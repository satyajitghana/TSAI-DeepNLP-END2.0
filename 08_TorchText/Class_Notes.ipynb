{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Class Notes.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satyajitghana/TSAI-DeepNLP-END2.0/blob/main/08_TorchText/Class_Notes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GIoefKyv4x4l"
      },
      "source": [
        "from torchtext.datasets import AG_NEWS"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vijftCqR45Xv"
      },
      "source": [
        "train_iter = AG_NEWS(split='train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwRgB50m5JO-",
        "outputId": "ce1703d4-3f85-49ed-c58e-75beed0adb9c"
      },
      "source": [
        "train_iter"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torchtext.data.datasets_utils._RawTextIterableDataset at 0x7fb3ade64510>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OW3Rvlz56B4o"
      },
      "source": [
        "x = next(train_iter)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c6_ct41R6G3w",
        "outputId": "7db9d14d-3500-4968-a8b8-8dc3ba9bde8d"
      },
      "source": [
        "type(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tuple"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9hmJTJyJ6I3d",
        "outputId": "458d876a-67aa-4122-de82-3df5f701da33"
      },
      "source": [
        "y, z = x\n",
        "print(z)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\which has a reputation for making well-timed and occasionally\\controversial plays in the defense industry, has quietly placed\\its bets on another part of the market.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTJbp49p6ZeJ",
        "outputId": "2d576683-57ae-4e97-a88d-f0ed77301056"
      },
      "source": [
        "for label, line in train_iter:\n",
        "  print(label, line)\n",
        "  break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\about the economy and the outlook for earnings are expected to\\hang over the stock market next week during the depth of the\\summer doldrums.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E6SdSezt6lVX",
        "outputId": "e3bf3cf7-91b3-4406-c276-312ad740bfee"
      },
      "source": [
        "for idx, (label, line) in enumerate(train_iter):\n",
        "  print(label, line)\n",
        "  if idx == 10:\n",
        "    break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3 Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\flows from the main pipeline in southern Iraq after\\intelligence showed a rebel militia could strike\\infrastructure, an oil official said on Saturday.\n",
            "3 Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.\n",
            "3 Stocks End Up, But Near Year Lows (Reuters) Reuters - Stocks ended slightly higher on Friday\\but stayed near lows for the year as oil prices surged past  #36;46\\a barrel, offsetting a positive outlook from computer maker\\Dell Inc. (DELL.O)\n",
            "3 Money Funds Fell in Latest Week (AP) AP - Assets of the nation's retail money market mutual funds fell by  #36;1.17 billion in the latest week to  #36;849.98 trillion, the Investment Company Institute said Thursday.\n",
            "3 Fed minutes show dissent over inflation (USATODAY.com) USATODAY.com - Retail sales bounced back a bit in July, and new claims for jobless benefits fell last week, the government said Thursday, indicating the economy is improving from a midsummer slump.\n",
            "3 Safety Net (Forbes.com) Forbes.com - After earning a PH.D. in Sociology, Danny Bazil Riley started to work as the general manager at a commercial real estate firm at an annual base salary of  #36;70,000. Soon after, a financial planner stopped by his desk to drop off brochures about insurance benefits available through his employer. But, at 32, \"buying insurance was the furthest thing from my mind,\" says Riley.\n",
            "3 Wall St. Bears Claw Back Into the Black  NEW YORK (Reuters) - Short-sellers, Wall Street's dwindling  band of ultra-cynics, are seeing green again.\n",
            "3 Oil and Economy Cloud Stocks' Outlook  NEW YORK (Reuters) - Soaring crude prices plus worries  about the economy and the outlook for earnings are expected to  hang over the stock market next week during the depth of the  summer doldrums.\n",
            "3 No Need for OPEC to Pump More-Iran Gov  TEHRAN (Reuters) - OPEC can do nothing to douse scorching  oil prices when markets are already oversupplied by 2.8 million  barrels per day (bpd) of crude, Iran's OPEC governor said  Saturday, warning that prices could fall sharply.\n",
            "3 Non-OPEC Nations Should Up Output-Purnomo  JAKARTA (Reuters) - Non-OPEC oil exporters should consider  increasing output to cool record crude prices, OPEC President  Purnomo Yusgiantoro said on Sunday.\n",
            "3 Google IPO Auction Off to Rocky Start  WASHINGTON/NEW YORK (Reuters) - The auction for Google  Inc.'s highly anticipated initial public offering got off to a  rocky start on Friday after the Web search company sidestepped  a bullet from U.S. securities regulators.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFdRSc-26rKi",
        "outputId": "67a50f65-6784-4344-a7d6-a5bd60b48676"
      },
      "source": [
        "next(train_iter)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3,\n",
              " \"Dollar Falls Broadly on Record Trade Gap  NEW YORK (Reuters) - The dollar tumbled broadly on Friday  after data showing a record U.S. trade deficit in June cast  fresh doubts on the economy's recovery and its ability to draw  foreign capital to fund the growing gap.\")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78F03o2N6uWz"
      },
      "source": [
        "# dataloader\n",
        "from torch.utils.data import DataLoader\n",
        "train_iter = AG_NEWS(split='train')\n",
        "dataloader = DataLoader(train_iter, batch_size = 8, shuffle = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "462hQNGp7f4l"
      },
      "source": [
        "class CustomDataset:\n",
        "  def __init__(self, csv):\n",
        "    # reading the CSV file \n",
        "    # in local variable called file\n",
        "    # self.current_location\n",
        "\n",
        "  def getitem():\n",
        "    # get current sentence location\n",
        "    current_location += 1\n",
        "    if current_location < self.len()\n",
        "      return file(current_location + 1)\n",
        "    else:\n",
        "      return StopIteration\n",
        "\n",
        "  def monitor_cuttent_location():\n",
        "    return current_location\n",
        "  \n",
        "  def len():\n",
        "    return len(csv)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3HXUt_C8JOl",
        "outputId": "167eea23-e0de-4c3c-bc24-66be1427b74c"
      },
      "source": [
        "next(iter(dataloader))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([3, 3, 3, 3, 3, 3, 3, 3]),\n",
              " (\"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\",\n",
              "  'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.',\n",
              "  \"Oil and Economy Cloud Stocks' Outlook (Reuters) Reuters - Soaring crude prices plus worries\\\\about the economy and the outlook for earnings are expected to\\\\hang over the stock market next week during the depth of the\\\\summer doldrums.\",\n",
              "  'Iraq Halts Oil Exports from Main Southern Pipeline (Reuters) Reuters - Authorities have halted oil export\\\\flows from the main pipeline in southern Iraq after\\\\intelligence showed a rebel militia could strike\\\\infrastructure, an oil official said on Saturday.',\n",
              "  'Oil prices soar to all-time record, posing new menace to US economy (AFP) AFP - Tearaway world oil prices, toppling records and straining wallets, present a new economic menace barely three months before the US presidential elections.',\n",
              "  'Stocks End Up, But Near Year Lows (Reuters) Reuters - Stocks ended slightly higher on Friday\\\\but stayed near lows for the year as oil prices surged past  #36;46\\\\a barrel, offsetting a positive outlook from computer maker\\\\Dell Inc. (DELL.O)',\n",
              "  \"Money Funds Fell in Latest Week (AP) AP - Assets of the nation's retail money market mutual funds fell by  #36;1.17 billion in the latest week to  #36;849.98 trillion, the Investment Company Institute said Thursday.\",\n",
              "  'Fed minutes show dissent over inflation (USATODAY.com) USATODAY.com - Retail sales bounced back a bit in July, and new claims for jobless benefits fell last week, the government said Thursday, indicating the economy is improving from a midsummer slump.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07txLTka8wcR"
      },
      "source": [
        "Torchtext has revamped the very basic components of the torchtext library, including vocab, word vectors, tokenizer. These are the basic data processing building blocks for a raw text strings.\n",
        "\n",
        "Here is an example for typical NLP data processing with tokenizer and vocabulary. The first step is to build a vocabulary with the raw training dataset. Here we use built in factory function build_vocab_from_iterator which accepts iterator that yield list or iterator of tokens. Users can also pass any special symbols to be added to the vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3Z2ulxx8LpH"
      },
      "source": [
        "from torchtext.vocab import uild_vocab_from_iterator\n",
        "from torchtext.data.utils import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "train_iter = AG_NEWS(split='train')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTsguiZw9bsg"
      },
      "source": [
        "def yield_fn(something):\n",
        "  for s in something:\n",
        "    yield s\n",
        "\n",
        "fn = yield_fn('rohan')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "htuin-g29k8E",
        "outputId": "fda8744f-9f29-43ee-a1b3-a0389aec26db"
      },
      "source": [
        "next(fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'r'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0FPhZAzt9qfh",
        "outputId": "ac707ae7-7ee9-4c76-b658-76a4b07cc007"
      },
      "source": [
        "next(fn), next(fn), next(fn)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('o', 'h', 'a')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IgmvQNou9wVY"
      },
      "source": [
        "def yield_tokens(data_iter):\n",
        "  for _, text in data_iter:\n",
        "    yield tokenizer(text)\n",
        "\n",
        "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=[\"<unk>\"], special_first=True)\n",
        "vocab.set_default_index(vocab[\"<unk>\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHWiyC7h-V3B",
        "outputId": "799be60b-fc86-4bf8-df70-b7b402ded64f"
      },
      "source": [
        "vocab(['here', 'is', 'an', 'example', 'theschoolofai'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[475, 21, 30, 5297, 0]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJlAsuo--YWI"
      },
      "source": [
        "text_pipeline = lambda x: vocab(tokenizer(x))\n",
        "label_pipeline = lambda x: int(x) - 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngCcDrxw--8I",
        "outputId": "cd2297a3-e5a3-4742-f2b2-de7a221c0244"
      },
      "source": [
        "text_pipeline('here is an example string')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[475, 21, 30, 5297, 3578]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UXxwioiE_BOi",
        "outputId": "46664b63-fde7-4e9b-c3e6-39072ab379b4"
      },
      "source": [
        "label_pipeline('10')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HENWYyqC_JTg"
      },
      "source": [
        "## Generate data batch and iterator\n",
        "\n",
        "torch.utils.data.DataLoader is recommended for PyTorch users. It works with a map-style dataset that implements the getitem() and len() protocols, and represents a map from indices/keys to data samples. It also works with an iterable dataset with the shuffle argument of False.\n",
        "\n",
        "Before sending to the model, collate_fn function works on a batch of samples generated from DataLoader. The input to collate_fn is a batch of data with the batch size in DataLoader, and collate_fn processes them according to the data processing pipelines declared previously. Pay attention here and make sure that collate_fn is declared as a top level def. This ensures that the function is available in each worker."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toRvmABH_FQW"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def collate_fn(batch):\n",
        "    src_batch, tgt_batch = [], []\n",
        "    for src_sample, tgt_sample in batch:\n",
        "        src_batch.append(text_transform[SRC_LANGUAGE](src_sample.rstrip(\"\\n\")))\n",
        "        tgt_batch.append(text_transform[TGT_LANGUAGE](tgt_sample.rstrip(\"\\n\")))\n",
        "\n",
        "    src_batch = pad_sequence(src_batch, padding_value=PAD_IDX)\n",
        "    tgt_batch = pad_sequence(tgt_batch, padding_value=PAD_IDX)\n",
        "    return src_batch, tgt_batch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FwqmBl0XEQCq"
      },
      "source": [
        "EmbeddingBag is a useful feature to consume sparse ids and produce embeddings. Later we are going to see examples where the text entries in the original batch input will be backed into a list and concatenated as a single tensor for the input of nn.EmbeddingBag.\n",
        "\n",
        "It is doing two things. The first step is to create an embedding and the second step is to reduce (sum/mean/max, according to the \"mode\" argument) the embedding output across dimension 0. So this is equivalent to torch.nn.functional.embedding, followed by torch.sum/mean/max. However, the conceptual two step process does not reflect how it's actually implemented. Since embedding_bag does not need to return the intermediate result, it doesn't actually generate a Tensor object for the embedding. It just goes straight to computing the reduction, pulling in the appropriate data from the weight argument according to the indices in the input argument. Avoiding the creation of the embedding Tensor allows for better performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tcOq-VwEP3q",
        "outputId": "2f36467b-1dcc-4b39-eed0-63502f3c8833"
      },
      "source": [
        "weight = torch.randn(3, 4)\n",
        "weight"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.4370, -1.1856,  0.7719, -0.7714],\n",
              "        [ 1.4535, -0.4327, -2.8715, -0.1674],\n",
              "        [-0.1786,  1.2191, -0.7373, -0.1179]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RrxYRbdjENlo",
        "outputId": "84c0e5fa-81bb-4426-e2b1-62914e6574b2"
      },
      "source": [
        "indices = torch.tensor([2, 1])\n",
        "indices"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrpWzb0FEhfR",
        "outputId": "da7b5e31-3ad7-49e4-9db2-99e4fe5d9553"
      },
      "source": [
        "embedding_dwork = torch.nn.functional.embedding(indices, weight)\n",
        "embedding_dwork"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.1786,  1.2191, -0.7373, -0.1179],\n",
              "        [ 1.4535, -0.4327, -2.8715, -0.1674]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O5V9rqATEszg",
        "outputId": "6f24a319-6312-4ae2-adf2-4fa61613e1da"
      },
      "source": [
        "embedding_dwork_mean = embedding_dwork.mean(dim=0, keepdim = True)\n",
        "embedding_dwork_mean"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.6375,  0.3932, -1.8044, -0.1427]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWBb1H1CE5Rw",
        "outputId": "6bceeb98-ddd7-41d6-e9db-081f8d65378e"
      },
      "source": [
        "embedding_bag = torch.nn.functional.embedding_bag(indices, weight, torch.tensor([0]), mode='mean')\n",
        "embedding_bag"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.6375,  0.3932, -1.8044, -0.1427]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohQhdZyjGfhW"
      },
      "source": [
        "In this example, the text entries in the original data batch input are packed into a list and concatenated as a single tensor for the input of nn.EmbeddingBag. The offset is a tensor of delimiters to represent the beginning index of the individual sequence in the text tensor. Label is a tensor saving the labels of individual text entries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9_x-TmMFQMU"
      },
      "source": [
        "def collate_batch(batch):\n",
        "    label_list, text_list, offsets = [], [], [0]\n",
        "    for (_label, _text) in batch:\n",
        "         label_list.append(label_pipeline(_label))\n",
        "         processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
        "         text_list.append(processed_text)\n",
        "         offsets.append(processed_text.size(0))\n",
        "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
        "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
        "    text_list = torch.cat(text_list)\n",
        "    return label_list.to(device), text_list.to(device), offsets.to(device)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wEM_gkTZHVQ6"
      },
      "source": [
        "train_iter = AG_NEWS(split='train')\n",
        "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsImovuCHfox"
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "class TextClassificationModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim, num_class):\n",
        "        super(TextClassificationModel, self).__init__()\n",
        "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
        "        self.fc = nn.Linear(embed_dim, num_class)\n",
        "        self.init_weights()\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.5\n",
        "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
        "        self.fc.bias.data.zero_()\n",
        "\n",
        "    def forward(self, text, offsets):\n",
        "        embedded = self.embedding(text, offsets)\n",
        "        return self.fc(embedded)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgyoyv61HtLo"
      },
      "source": [
        "The AG_NEWS dataset has four labels and therefore the number of classes is four.\n",
        "\n",
        "::\n",
        "\n",
        "1 : World 2 : Sports 3 : Business 4 : Sci/Tec\n",
        "\n",
        "We build a model with the embedding dimension of 64. The vocab size is equal to the length of the vocabulary instance. The number of classes is equal to the number of labels,"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bNPmNGyyHrEH"
      },
      "source": [
        "train_iter = AG_NEWS(split='train')\n",
        "num_class = len(set([label for (label, text) in train_iter]))\n",
        "vocab_size = len(vocab)\n",
        "emsize = 64\n",
        "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zb9CGpeAH5P0",
        "outputId": "a234960a-c552-4e1b-d912-036708091624"
      },
      "source": [
        "set([1, 1,1, 1, 1, 1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "amvOWAgEH6Ut"
      },
      "source": [
        "import time\n",
        "\n",
        "def train(dataloader):\n",
        "    model.train()\n",
        "    total_acc, total_count = 0, 0\n",
        "    log_interval = 500\n",
        "    start_time = time.time()\n",
        "\n",
        "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "        optimizer.zero_grad()\n",
        "        predited_label = model(text, offsets)\n",
        "        loss = criterion(predited_label, label)\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
        "        optimizer.step()\n",
        "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "        total_count += label.size(0)\n",
        "        if idx % log_interval == 0 and idx > 0:\n",
        "            elapsed = time.time() - start_time\n",
        "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
        "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
        "                                              total_acc/total_count))\n",
        "            total_acc, total_count = 0, 0\n",
        "            start_time = time.time()\n",
        "\n",
        "def evaluate(dataloader):\n",
        "    model.eval()\n",
        "    total_acc, total_count = 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
        "            predited_label = model(text, offsets)\n",
        "            loss = criterion(predited_label, label)\n",
        "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
        "            total_count += label.size(0)\n",
        "    return total_acc/total_count"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmA7KbtTJFTU"
      },
      "source": [
        "Split the dataset and run the model Since the original AG_NEWS has no valid dataset, we split the training dataset into train/valid sets with a split ratio of 0.95 (train) and 0.05 (valid). Here we use torch.utils.data.dataset.random_split https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split__ function in PyTorch core library.\n",
        "\n",
        "CrossEntropyLoss https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss__ criterion combines nn.LogSoftmax() and nn.NLLLoss() in a single class. It is useful when training a classification problem with C classes. SGD https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html__ implements stochastic gradient descent method as the optimizer. The initial learning rate is set to 5.0. StepLR https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR__ is used here to adjust the learning rate through epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xFd5yH0uJCsF",
        "outputId": "03722f69-8c7e-4f4e-d44b-f0d7613e8e3a"
      },
      "source": [
        "from torch.utils.data.dataset import random_split\n",
        "from torchtext.data.functional import to_map_style_dataset\n",
        "# Hyperparameters\n",
        "EPOCHS = 10 # epoch\n",
        "LR = 5  # learning rate\n",
        "BATCH_SIZE = 64 # batch size for training\n",
        "  \n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
        "total_accu = None\n",
        "train_iter, test_iter = AG_NEWS()\n",
        "train_dataset = to_map_style_dataset(train_iter)\n",
        "test_dataset = to_map_style_dataset(test_iter)\n",
        "num_train = int(len(train_dataset) * 0.95)\n",
        "split_train_, split_valid_ = \\\n",
        "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
        "\n",
        "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
        "                              shuffle=True, collate_fn=collate_batch)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
        "                             shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    epoch_start_time = time.time()\n",
        "    train(train_dataloader)\n",
        "    accu_val = evaluate(valid_dataloader)\n",
        "    if total_accu is not None and total_accu > accu_val:\n",
        "      scheduler.step()\n",
        "    else:\n",
        "       total_accu = accu_val\n",
        "    print('-' * 59)\n",
        "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
        "          'valid accuracy {:8.3f} '.format(epoch,\n",
        "                                           time.time() - epoch_start_time,\n",
        "                                           accu_val))\n",
        "    print('-' * 59)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test.csv: 1.86MB [00:00, 42.1MB/s]                  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "| epoch   1 |   500/ 1782 batches | accuracy    0.689\n",
            "| epoch   1 |  1000/ 1782 batches | accuracy    0.859\n",
            "| epoch   1 |  1500/ 1782 batches | accuracy    0.873\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   1 | time:  7.69s | valid accuracy    0.880 \n",
            "-----------------------------------------------------------\n",
            "| epoch   2 |   500/ 1782 batches | accuracy    0.896\n",
            "| epoch   2 |  1000/ 1782 batches | accuracy    0.898\n",
            "| epoch   2 |  1500/ 1782 batches | accuracy    0.903\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   2 | time:  7.52s | valid accuracy    0.894 \n",
            "-----------------------------------------------------------\n",
            "| epoch   3 |   500/ 1782 batches | accuracy    0.913\n",
            "| epoch   3 |  1000/ 1782 batches | accuracy    0.913\n",
            "| epoch   3 |  1500/ 1782 batches | accuracy    0.915\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   3 | time:  7.49s | valid accuracy    0.899 \n",
            "-----------------------------------------------------------\n",
            "| epoch   4 |   500/ 1782 batches | accuracy    0.927\n",
            "| epoch   4 |  1000/ 1782 batches | accuracy    0.920\n",
            "| epoch   4 |  1500/ 1782 batches | accuracy    0.923\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   4 | time:  7.44s | valid accuracy    0.907 \n",
            "-----------------------------------------------------------\n",
            "| epoch   5 |   500/ 1782 batches | accuracy    0.931\n",
            "| epoch   5 |  1000/ 1782 batches | accuracy    0.928\n",
            "| epoch   5 |  1500/ 1782 batches | accuracy    0.930\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   5 | time:  7.55s | valid accuracy    0.904 \n",
            "-----------------------------------------------------------\n",
            "| epoch   6 |   500/ 1782 batches | accuracy    0.943\n",
            "| epoch   6 |  1000/ 1782 batches | accuracy    0.939\n",
            "| epoch   6 |  1500/ 1782 batches | accuracy    0.945\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   6 | time:  7.53s | valid accuracy    0.911 \n",
            "-----------------------------------------------------------\n",
            "| epoch   7 |   500/ 1782 batches | accuracy    0.945\n",
            "| epoch   7 |  1000/ 1782 batches | accuracy    0.944\n",
            "| epoch   7 |  1500/ 1782 batches | accuracy    0.943\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   7 | time:  7.43s | valid accuracy    0.910 \n",
            "-----------------------------------------------------------\n",
            "| epoch   8 |   500/ 1782 batches | accuracy    0.947\n",
            "| epoch   8 |  1000/ 1782 batches | accuracy    0.946\n",
            "| epoch   8 |  1500/ 1782 batches | accuracy    0.944\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   8 | time:  7.57s | valid accuracy    0.911 \n",
            "-----------------------------------------------------------\n",
            "| epoch   9 |   500/ 1782 batches | accuracy    0.945\n",
            "| epoch   9 |  1000/ 1782 batches | accuracy    0.945\n",
            "| epoch   9 |  1500/ 1782 batches | accuracy    0.947\n",
            "-----------------------------------------------------------\n",
            "| end of epoch   9 | time:  7.53s | valid accuracy    0.910 \n",
            "-----------------------------------------------------------\n",
            "| epoch  10 |   500/ 1782 batches | accuracy    0.948\n",
            "| epoch  10 |  1000/ 1782 batches | accuracy    0.946\n",
            "| epoch  10 |  1500/ 1782 batches | accuracy    0.945\n",
            "-----------------------------------------------------------\n",
            "| end of epoch  10 | time:  7.46s | valid accuracy    0.910 \n",
            "-----------------------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lbuiamSPJU4J",
        "outputId": "1dd7a3bd-30b9-45c6-96bc-e45703a04c3e"
      },
      "source": [
        "print('Checking the results of test dataset.')\n",
        "accu_test = evaluate(test_dataloader)\n",
        "print('test accuracy {:8.3f}'.format(accu_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Checking the results of test dataset.\n",
            "test accuracy    0.908\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTha93LgMuUW",
        "outputId": "2b8c8a50-bea2-4d55-ee92-c920c1517e9d"
      },
      "source": [
        "ag_news_label = {1: \"World\",\n",
        "                 2: \"Sports\",\n",
        "                 3: \"Business\",\n",
        "                 4: \"Sci/Tec\"}\n",
        "\n",
        "def predict(text, text_pipeline):\n",
        "    with torch.no_grad():\n",
        "        text = torch.tensor(text_pipeline(text))\n",
        "        output = model(text, torch.tensor([0]))\n",
        "        return output.argmax(1).item() + 1\n",
        "\n",
        "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
        "    enduring the season’s worst weather conditions on Sunday at The \\\n",
        "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
        "    considering the wind and the rain was a respectable showing. \\\n",
        "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
        "    was another story. With temperatures in the mid-80s and hardly any \\\n",
        "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
        "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
        "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
        "    was even more impressive considering he’d never played the \\\n",
        "    front nine at TPC Southwind.\"\n",
        "\n",
        "model = model.to(\"cpu\")\n",
        "\n",
        "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, text_pipeline)])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This is a Sports news\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7b0Zuy_Mz_y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}